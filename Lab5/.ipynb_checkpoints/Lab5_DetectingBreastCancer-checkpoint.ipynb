{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Detecting Breast Cancer using Logistic Regression\n",
    "\n",
    "## Learning Objectives\n",
    "* Applying Linear Classification to Breast Cancer Prediction\n",
    "* Understanding Logistic Regression\n",
    "* Model Evaluation for Classification\n",
    "\n",
    "## Outline\n",
    "1. [Implementing Linear Regression](#1.-Implementing-Logistic-Regression)\n",
    "    1. [Preparing the Toy Dataset](#Preparing-the-Toy-Dataset)\n",
    "    2. [Consolidating Model Parameters](#Consolidating-Model-Parameters)\n",
    "    3. [Computing Class Probabilities](#Computing-Class-Probabilities)\n",
    "    4. [Fitting the Model](#Fitting-the-Model)\n",
    "    5. [Making Predictions](#Making-Predictions)\n",
    "    6. [Visualizing the Result](#Visualizing-the-Result)\n",
    "2. [Predicting Breast Cancer](#2.-Predicting-Breast-Cancer)\n",
    "    1. [Understanding the Dataset](#Understanding-the-Dataset)\n",
    "    2. [Training the Logistic Regression Model](#Training-the-Logistic-Regression-Model)\n",
    "    3. [Predict and Evaluate Results](#Predict-and-Evaluate-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Lab\n",
    "Doctors at the Wash U Medical School need your help in finding a fast, reliable method of detecting the malignancy of breast cancer tumors for their patients. Thankfully, Dr. William H. Wolberg from the University of Wisconsin has provided a dataset of hundreds of `benign` and `malignant` cases at their hospital. By the end of this lab, you will know how to build an accurate classification model to perform this task. We will begin by revising and implementing the logistic regression classifier and then train and test it on the breast cancer dataset. For evaluation we will introduce some new metrics that are specific to classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing Logistic Regression\n",
    "\n",
    "An implementation of logistic regression has several components and we will walk through all of them step-by-step. Before we take a look at cancer data, let's take a step back and use a toy dataset for our implementation. Since we will only be considering binary (+1/-1) classification today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.util import load_toy\n",
    "\n",
    "X, y = load_toy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the data that we are trying to model.\n",
    "\n",
    "**Try this!** Create a scatter plot that shows `X` ($x_1$ and $x_2$) and `y` in different colors and with different marker shapes. Include appropriate axes labels, a legend, and a title. Check your plot with your neighbors and a TA (for a quick sanity check). `Hint` You can make use of the `label` keyword argument of any `Matplotlib` plotting function with `plt.legend` to automatically take care of your legend labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utility.util import configure_plots\n",
    "\n",
    "configure_plots()\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating Model Parameters\n",
    "\n",
    "Before we start we'll apply the trick of consolidating the mdoel parameters $b$ and $w$ as was described in lecture. In our discussions so far we have represented models with the general formula: $$y = wx + b.$$ In this representation, we notate $w$ and $b$ as distinct entities with separate names. However, it is also very common to see all of a models parameters referred to as a single unit called $\\theta$, $\\beta$, or $w$, amongst other names, depending on the context.\n",
    "\n",
    "> _For the curious_: In statistics, it is common to see model parameters referred to as $\\beta_0$, $\\beta_1$, etc.; and, in ML, it is more common to see them referred to as $w_0$, $w_1$, etc.\n",
    "\n",
    "This is a valid thing to do because the bias or constant term $b$ is really just another parameter of the model. Practically, however, this means that we will need to change the way we define $w$ and $X$ in order to keep the math straight.\n",
    "\n",
    "The way to do this is by prepending $b$ to our weights $w$ as $w_0$ so that, $$w = [b, w_1, w_2, \\ldots] = [w_0, w_1, w_2, \\ldots].$$\n",
    "\n",
    "Accordingly, we must also adjust $X$ in order to keep the math (via dot product) consistent. We can do this by adding a $1$ to each observation so that for each data point, $x_i$, we have $$x_i = [1, x_{i, 0}, x_{i, 1}, \\ldots, x_{i, d}].$$\n",
    "\n",
    "These adjustments result in\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_i &= w^\\top x_i \\\\\n",
    "    &= w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d \\\\\n",
    "    &= b + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Going forward, expect to see more of this notation, but remember that it is the equivalent to our initial notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the following function so that it returns `X_aug`, which is `X` augmented with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    '''Returns X with ones prepended to each observation'''\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    assert X.shape[0] == X_aug.shape[0], \\\n",
    "        'Adding an intercept should NOT change the number of observations'\n",
    "    assert X.shape[1] + 1 == X_aug.shape[1], \\\n",
    "        'Adding an intercept should increase the number of features by exactly 1.'\n",
    "    \n",
    "    return X_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should return `X` with now three columns instead of two, where the first column is one for each data point.\n",
    "\n",
    "**Try this!** Slice and evaluate the first 5 augmented data inputs from `X_aug`. Is this what you expected to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = add_intercept(X)\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Class Probabilities\n",
    "\n",
    "As you have seen during class, the logistic regression model defines the probability that the label of a data point is `+1` given its features:\n",
    "$$P(y=+1 \\mid x) = \\frac{1}{1 + e^{-w^{\\top}x}}$$ And, by the laws of probability, the probability that the label of a data point is `-1` is simply the complement, or $$P(y =-1 \\mid x) = 1 - P(y=+1 \\mid x)$$\n",
    "\n",
    "Intuitively, the model takes a score $s(x) =-w^{\\top}x$ that can be in the range $[-\\infty, \\infty]$ and interprets it as a probability using the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `sigmoid` function so that it computes $\\text{sigmoid}(s)$ using NumPy operations. Note that `s` can be either a scalar or a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(score):\n",
    "    '''Computes the sigmoid value of s'''\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your implementation by plotting it. If you did everything correctly, the y-axis will be between 0 and 1 and the function will be (point) symmetric with respect to the center point $(0,0.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = np.linspace(-5, 5, 101)\n",
    "plt.plot(xs, sigmoid(xs))\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('sigmoid(score)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model\n",
    "\n",
    "Training the logistic regression model means learning the weights, $w$, by maximizing [**likelihood**](https://en.wikipedia.org/wiki/Likelihood_function). Wikipedia says that \"Likelihood describes the plausibility, given specific observed data, of a parameter value of the statistical model which is assumed to describe that data.\" In other words, a likelihood is a measure of how likely a model parameterized by $w$ could have produced the data $(X, y)$.\n",
    "\n",
    "The likelihood of _one particular data point_ $(x^*, y^*)$ is given as $L(w \\mid x^*, y^*) = P(y=y^* \\mid x^*, w)$ such that\n",
    "\n",
    "$$\n",
    "P(y^* \\mid x^*, w) = \\left \\{\n",
    "    \\begin{array}{lr}\n",
    "    \\frac{1}{1 + e^{-w^{T}x^*}} \\text{ for } y^* = +1\\\\\n",
    "    1-\\frac{1}{1 + e^{-w^{T}x^*}} \\text{ for }  y^* = -1\n",
    "    \\end{array}\n",
    "    \\right .\n",
    "$$\n",
    "\n",
    "By the laws of what is called a [_joint probability distribution_](https://en.wikipedia.org/wiki/Joint_probability_distribution) (of independent random variables), the likelihood for the entire training dataset is given as the product of the data-point likelihoods: \n",
    "$$L(w \\mid  X, y) = \\prod_{i=1}^n L(w \\mid x_i, y_i).$$\n",
    "\n",
    "Now, what we want is to find the model parameters that fit our training dataset the _best_. This means we need to look for the model parameters that maximize this likelihood! This is another **optimization problem**, similar to the one we solved for linear regression. However, unlike that of linear regression, this one has no [closed-form](https://en.wikipedia.org/wiki/Closed-form_expression) solution â€” we have to use numerical optimization to solve it.\n",
    "\n",
    "The algorithm to solve this is an iterative hill climbing technique also known as [_gradient ascent_](https://en.wikipedia.org/wiki/Gradient_descent) (or _descent_ if you have a minimization problem). \n",
    "> **Intuition**:  The algorithm starts with a random set of parameters and then iteratively updates the parameters following the direction of the gradient until the gradient is 0. <br />\n",
    "> **Challenge**: Check out the implementation we provided for you in `utility/util.py` (After doing the math (which requires _some_ calculus, everything turns out surprisingly simple - like you'll only need \"five\" lines of code.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a logistic regression model with our Iris data. As we mentioned previously, the optimization process for finding $w$ is a bit out of scope for our class, so instead of implementing it together, we have provided our own implementation, `fit`, for you to use.\n",
    "\n",
    "**Try this!** Create a train/test split of `X_aug` and `y` (using the 80:20 ratio and a `random_state` of 3) and then use `fit` to train the model. Store the output of `fit` in the variable `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.util import optimize_logistic as fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Given weights $w$ and points $X$ we can make predictions by computing $P(y=+1 | x)$. With this probability and a threshold, we can produce a predicted label such that if $P(y=+1 | x) > \\text{threshold}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `predict_probability` function below so that it computes $P(y=+1 | x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(X, w):\n",
    "    '''\n",
    "    Computes the predicted probability of points in X given model parameters W\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    assert np.all(0 <= p) and np.all(p <= 1), 'Probability must be between 0 and 1'\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Complete the `predict` function below so that it computes the predicted `labels` of points in `X` given parameters `w`. The threshold is provided as an argument to the function. Make sure that your labels are either `+1` or `-1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, threshold=0.5):\n",
    "    '''\n",
    "    Given the inputs X and model paramters w, turns the probability \n",
    "    of being +1 into a hard label using the threshold (default = 0.5)\n",
    "    '''\n",
    "    assert np.all(X[:, 0] == 1), 'X needs to be augmented with a column of ones!'\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    assert isinstance(labels, np.ndarray), 'Labels should be returned as a NumPy array'\n",
    "    assert labels.shape[0] == N, 'There should be the same number of labels as their are points in X'\n",
    "    assert np.all(np.unique(labels) == np.array([-1, 1])), 'Labels should be either +1 or -1'\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation use the following paramters, provided by an _oracle_. Make sure you understand what each of them corresponds to. Your predictions for the first 4 data points should positive and the 5th data point should be predicted negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_w = np.array([-0.09,  0.58, -0.80])\n",
    "predict(X_aug[:5,:], sample_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's take some time to consider how we might decide what decision threshold to use.\n",
    "\n",
    "**Write-up!** Why does it make sense to use 0.5? Can you think of any scenarios where you might want to use a different threshold?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Result\n",
    "We can now visualize the decision boundary and the prediction results. In the cell below, we have provided the code for scattering the training set as well as plotting the decision boundary. Fill in the scatter plots for the testing set in addition to adding plot description components.\n",
    "\n",
    "**Try this!** Create a scatter plot that shows _setosa_ and _non-setosa_ in different colors and with different marker shapes for the testing sets. We have provided `y_pred` which contains the predicted labels for the points in `X_test`. Include appropriate axes labels, a legend, and a title. Check your plot with your neighbors and a TA (for a quick sanity check). `Hint` you will make 4 calls to `plt.scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, w)\n",
    "\n",
    "# training data scatters\n",
    "plt.scatter(X_train[y_train > 0, 1], X_train[y_train > 0, 2], marker='^', color='orange', label='Training Setosa (+1)')\n",
    "plt.scatter(X_train[y_train < 0, 1], X_train[y_train < 0, 2], marker='s', label='Training Other (-1)')\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Plot the decision boundary (pretty complicated plot - feel free to ignore)\n",
    "A, B = np.meshgrid(np.linspace(X_aug[:, 1].min(), X_aug[:, 1].max()),\n",
    "                   np.linspace(X_aug[:, 2].min(), X_aug[:, 2].max()))\n",
    "xs = add_intercept(np.c_[A.ravel(), B.ravel()])\n",
    "ps = predict_probability(xs, w).reshape(A.shape)\n",
    "plt.contour(A, B, ps, [0.5], linewidths=1, colors='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predicting Breast Cancer\n",
    "Since all of this looks pretty good now, we can trun to our actual application for today: predicitng whether a patient has breas cancer or not. First, let's take a quick look at the data from the University of Wisconsin. Each data point contains information about the breast cancer cells of a single patient derived from a digitized image of a fine needle aspirate (FNA) of a breast mass, similar to those: \n",
    "\n",
    "![MoMA](utility/pics/fna.jpg)\n",
    "\n",
    "Note that we do not have the image data, but the features capture the cell shapes and we also have the diagnosis (either `malignant` or `benign`), which we will treat as the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset\n",
    "\n",
    "Let's take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a second to explore `data`.\n",
    "\n",
    "**Try this!** In the following cell, evaluate each of the fields in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Just for safe-keeping, how many data points are there in this dataset? How many features? What are the names of the features? What are the names of the classes and how are they encoded in the labels?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with what the data looks like, let's pull out what we need from it.\n",
    "\n",
    "**Try this!** Pull out the input data `X` and label (classification) data `y` from `data`. Make sure that the labels in `y` are either `+1` for `malignant` or `-1` for `benign` tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "N, d = X.shape\n",
    "f'There are {N} data points with {d} features each.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of the classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "print(classes)\n",
    "\n",
    "plt.bar(['benign (-1)', 'malignant (+1)'], counts)\n",
    "plt.title('Class Distribution - Breast Cancer Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** What do you notice?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression Model\n",
    "\n",
    "Now, you will train the classifier and then evaluate its performance. \n",
    "\n",
    "**Try this!** First, let's create the train-test split (use `random_state=42`). However, as you hopefully noticed above, our dataset does not have a balanced class distribution. So, when creating the train/test (in the usual 80/20 ratio) split use the argument `stratify=y` to get the same class distribution in the training and the testing set. Remember to augment your data with `add_intercept`.\n",
    "> **Discuss with your neighbors:** Why is this relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# your code here\n",
    "\n",
    "assert X_train.shape[1] == X.shape[1] + 1, 'Remeber to augment your data'\n",
    "\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(counts)\n",
    "print(classes)\n",
    "plt.bar(['benign (-1)','malignant (+1)'],counts)\n",
    "plt.title('Training Class Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** In the cell below, use `fit` with `X_train` and `y_train` and store the resulting weights in `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Evaluate Results\n",
    "Use the classifier to predict the class labels on the test data.\n",
    "\n",
    "**Try this** Use `predict` with `X_test` and weights `w` to produce predictions `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's count the mistakes and compute the error rate (out of 100%). \n",
    "\n",
    "**Try this!** Complete the `accuracy` function below so that it computes the accuracy of the `predictions` relative to the `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    '''Computes the accuracy of the predictions'''\n",
    "    \n",
    "    N = labels.shape[0]\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Accuracy: {accuracy(y_pred, y_test) * 100:0.5}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good - we did not make a lot of mistakes. However, let's analyze this more closely. For classification problems we can instead of just counting mistakes look at which kind of mistakes we made. \n",
    "\n",
    "**Write-up!** What are the two different kinds of mistakes we can make for breast cancer prediction?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "Recall that in `Lab3` we counted _false positives_ and _false negatives_. Relate those measures to our application and your answer above. Now, let's compute those, in fact, let's compute the entire confusion matrix. `scikit-learn` comes with a metric that does that for us.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "classes = [+1, -1]\n",
    "class_names = ['malignant (+1)', 'benign (-1)']\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize this to make the results easier to interpret: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the color matrix\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "\n",
    "# Label the axes\n",
    "classes = [+1, -1]\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "\n",
    "# Print the counts\n",
    "thresh = cnf_matrix.max() / 2.\n",
    "for i in range(cnf_matrix.shape[0]):\n",
    "    for j in range(cnf_matrix.shape[1]):\n",
    "        plt.text(j, i, format(cnf_matrix[i, j], 'd'), FontSize='15',\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Discuss the results with your neighbors. Are you happy with your results? What can you learn from this visualization? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
